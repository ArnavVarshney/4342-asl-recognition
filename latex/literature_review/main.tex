\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

    \title{Accelerating AI: A Comparative Analysis of GPUs, TPUs, and their Performance}

    \author{\IEEEauthorblockN{Abel Haris Harsono\IEEEauthorrefmark{1},
        Arnav Varshney\IEEEauthorrefmark{2}, Filbert David Tejalaksana\IEEEauthorrefmark{3}}
    \IEEEauthorblockA{\textit{Department of Electrical and Electronic Engineering} \\
    \textit{The University of Hong Kong}\\
    Hong Kong \\
    \{ u3583434\IEEEauthorrefmark{1}, arnav\IEEEauthorrefmark{2}, f1lbert\IEEEauthorrefmark{3} \}@connect.hku.hk}
    }

    \maketitle

    \begin{abstract}
        This document is a model and instructions for \LaTeX.
        This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.].
        *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,
        or Math in Paper Title or Abstract.
    \end{abstract}

    \begin{IEEEkeywords}
        component, formatting, style, styling, insert
    \end{IEEEkeywords}


    \section{Introduction}
    \label{sec:introduction}

    Deep learning algorithms, particularly neural networks with millions of parameters, have demonstrated exceptional capabilities in solving complex tasks such as image recognition, speech synthesis, and language translation.
    However, the computational demands of training and deploying these models pose significant challenges for traditional CPUs (Central Processing Units) and even Graphics Processing Units (GPUs).
    To address this issue, specialized hardware accelerators specifically designed for deep learning workloads have gained immense popularity such as Google's Tensor Processing Unit (TPU).

    Google's TPUv4, Nvidia's A100 GPU, and AMD's Radeon VII GPU have emerged as prominent contenders in the market, representing cutting-edge architectures tailored to accelerate deep learning tasks.
    While both platforms aim to deliver high performance, they adopt different approaches to achieve optimal efficiency.
    The TPU is a custom-built application-specific integrated circuit (ASIC) designed by Google, while the A100 leverages the Ampere architecture from Nvidia.

    This paper presents a detailed comparative analysis of Google's TPUv4 and Nvidia's A100, focusing on their architectural characteristics, computational capabilities, memory systems, and power efficiency.
    Furthermore,
    the performance of several accelerators (both GPUs and TPUs)
    is evaluated across computation-intensive operations (CIOs) and a range of deep learning benchmarks.
    A brief discussion on the impact of each platform's design choices on the overall performance and usability is also provided.

    Finally, the paper is concluded with a brief outline on potential avenues for future research in the field of deep learning hardware accelerators.
    The results of this comparative study will aid researchers, developers,
    and practitioners
    in making informed decisions
    when selecting the most appropriate platform for their specific deep learning requirements.
    Furthermore,
    the problems
    revealed in the evaluation results would prove useful in hardware and software design iterations for further optimization.


    \section{Nvidia A100 Tensor Core GPU}
    \label{sec:nvidia-a100-tensor-core-gpu}
    \input{nvidia-a100}


    \section{Tensor Processing Unit}
    \label{sec:tensor-processing-unit}
    \input{tpu}


    \section{Performance Evaluation}
    \label{sec:performance-evaluation}
    \input{performance}


    \label{sec:refs}
    \input{refs}


    \section{Contribution Statement}
    \label{sec:contribution-statement}
    \begin{itemize}
        \item Abel Haris Harsono: 33.3\%
        \item Arnav Varshney: 33.3\%
        \item Filbert David Tejalaksana: 33.3\%
    \end{itemize}
\end{document}
