\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

    \title{Accelerating AI: A Comparative Analysis of GPUs, TPUs, and their Performance}

    \author{\IEEEauthorblockN{Abel Haris Harsono\IEEEauthorrefmark{1},
        Arnav Varshney\IEEEauthorrefmark{2}, Filbert David Tejalaksana\IEEEauthorrefmark{3}}
    \IEEEauthorblockA{\textit{Department of Electrical and Electronic Engineering} \\
    \textit{The University of Hong Kong}\\
    Hong Kong \\
    \{ u3583434\IEEEauthorrefmark{1}, arnav\IEEEauthorrefmark{2}, f1lbert\IEEEauthorrefmark{3} \}@connect.hku.hk}
    }

    \maketitle

    \begin{abstract}
        This document is a model and instructions for \LaTeX.
        This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.].
        *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,
        or Math in Paper Title or Abstract.
    \end{abstract}

    \begin{IEEEkeywords}
        component, formatting, style, styling, insert
    \end{IEEEkeywords}


    \section{Introduction}
    \label{sec:introduction}

    \{Generated from GPT; a placeholder\}
    Deep learning algorithms, particularly neural networks with millions of parameters, have demonstrated exceptional capabilities in solving complex tasks such as image recognition, speech synthesis, and language translation.
    However, the computational demands of training and deploying these models pose significant challenges for traditional CPUs (Central Processing Units) and even GPUs.
    To address this issue, specialized hardware accelerators specifically designed for deep learning workloads have gained immense popularity.

    Google's Tensor Processing Unit (TPU) and Nvidia's A100 GPU have emerged as two prominent contenders in the market, representing cutting-edge architectures tailored to accelerate deep learning tasks.
    While both platforms aim to deliver high performance, they adopt different approaches to achieve optimal efficiency.
    The TPU is a custom-built integrated circuit designed by Google, while the A100 leverages the latest GPU architecture from Nvidia.
    Understanding the unique features and trade-offs of each platform is crucial for researchers and practitioners to make informed decisions when selecting hardware accelerators for their deep learning applications.

    This paper presents a detailed comparative analysis of Google's TPU and Nvidia's A100, focusing on their architectural characteristics, computational capabilities, memory systems, and power efficiency.
    Furthermore, we explore their performance across a range of deep learning benchmarks and discuss the impact of each platform's design choices on the overall performance and usability.
    The analysis considers factors such as peak performance, memory bandwidth, precision support, and software ecosystem to provide a comprehensive evaluation of the strengths and weaknesses of each platform.

    The remainder of this document is structured as follows: Section II provides an overview of the architectural features of Google's TPU and Nvidia's A100.
    Section III discusses the performance metrics and benchmarks used to evaluate the platforms.
    Section IV presents a comparative analysis of the TPU and A100, highlighting their similarities and differences.
    Section V discusses the implications of the findings and provides recommendations for selecting the most suitable platform for different deep learning applications.
    Finally, Section VI concludes the paper and outlines potential avenues for future research in the field of deep learning hardware accelerators.

    In summary, this paper aims to contribute to the body of knowledge surrounding hardware accelerators for deep learning by providing an in-depth analysis of Google's TPU and Nvidia's A100.
    The results of this comparative study will aid researchers, developers, and practitioners in making informed decisions when selecting the most appropriate platform for their specific deep learning requirements.


    \section{Nvidia A100 Tensor Core GPU}
    \label{sec:nvidia-a100-tensor-core-gpu}
    \input{nvidia-a100}

    \section{Tensor Processing Unit}
    \label{sec:tensor-processing-unit}
    \input{tpu}

    \section{Contribution Statement}
    \label{sec:contribution-statement}
    \begin{itemize}
        \item Abel Haris Harsono: 33.3\%
        \item Arnav Varshney: 33.3\%
        \item Filbert David Tejalaksana: 33.3\%
    \end{itemize}
\end{document}
