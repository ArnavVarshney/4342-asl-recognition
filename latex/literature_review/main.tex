\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

    \title{Accelerating AI: A Comparative Analysis of GPUs, TPUs, and their Performance}

    \author{\IEEEauthorblockN{Abel Haris Harsono\IEEEauthorrefmark{1},
        Arnav Varshney\IEEEauthorrefmark{2}, Filbert David Tejalaksana\IEEEauthorrefmark{3}}
    \IEEEauthorblockA{\textit{Department of Electrical and Electronic Engineering} \\
    \textit{The University of Hong Kong}\\
    Hong Kong \\
    \{ u3583434\IEEEauthorrefmark{1}, arnav\IEEEauthorrefmark{2}, f1lbert\IEEEauthorrefmark{3} \}@connect.hku.hk}
    }

    \maketitle

    \begin{abstract}
        This paper presents a comprehensive comparative analysis of Google's Tensor Processing Unit (TPUv4) and Nvidia's A100 GPU, the two leading hardware accelerators designed for deep learning workloads and AI applications.
        We delve into their architectural characteristics, computational capabilities, memory systems, and power efficiency.
        The performance of these accelerators is evaluated across computation-intensive operations and a range of deep learning benchmarks.
        The impact of each platform's design choices on overall performance and usability was also studied.
        The findings of this study will assist researchers, developers, and practitioners in selecting the most suitable platform for their specific deep learning needs.
        Furthermore, the evaluation results could guide future hardware and software design iterations for further optimization.
    \end{abstract}

    \begin{IEEEkeywords}
        Tensor Processing Unit, TPUv4, Nvidia A100, GPU, Deep Learning, Hardware Accelerators, Computational Capabilities, Memory Systems, Power Efficiency, Performance Evaluation, Platform Design, Neural Architecture Search, SparseCore, Optical Switches, Embedding
    \end{IEEEkeywords}


    \section{Introduction}
    \label{sec:introduction}

    Deep learning algorithms, particularly neural networks with millions of parameters, have demonstrated exceptional capabilities in solving complex tasks such as image recognition, speech synthesis, and language translation.
    However, the computational demands of training and deploying these models pose significant challenges for traditional CPUs (Central Processing Units) and even Graphics Processing Units (GPUs).
    To address this issue, specialized hardware accelerators specifically designed for deep learning workloads have gained immense popularity such as Google's Tensor Processing Unit (TPU).

    Google's TPUv4, Nvidia's A100 GPU, and AMD's Radeon VII GPU have emerged as prominent contenders in the market, representing cutting-edge architectures tailored to accelerate deep learning tasks.
    While both platforms aim to deliver high performance, they adopt different approaches to achieve optimal efficiency.
    The TPU is a custom-built application-specific integrated circuit (ASIC) designed by Google, while the A100 leverages the Ampere architecture from Nvidia.

    This paper presents a detailed comparative analysis of Google's TPUv4 and Nvidia's A100, focusing on their architectural characteristics, computational capabilities, memory systems, and power efficiency.
    Furthermore,
    the performance of several accelerators (both GPUs and TPUs)
    is evaluated across computation-intensive operations (CIOs) and a range of deep learning benchmarks.
    A brief discussion on the impact of each platform's design choices on the overall performance and usability is also provided.

    Finally, the paper is concluded with a brief outline on potential avenues for future research in the field of deep learning hardware accelerators.
    The results of this comparative study will aid researchers, developers,
    and practitioners
    in making informed decisions
    when selecting the most appropriate platform for their specific deep learning requirements.
    Furthermore,
    the problems
    revealed in the evaluation results would prove useful in hardware and software design iterations for further optimization.


    \section{Nvidia A100 Tensor Core GPU}
    \label{sec:nvidia-a100-tensor-core-gpu}
    \input{nvidia-a100}


    \section{Tensor Processing Unit}
    \label{sec:tensor-processing-unit}
    \input{tpu}


    \section{Performance Evaluation}
    \label{sec:performance-evaluation}
    \input{performance}

    \vspace{15pt}


    \section{Applications}
    \label{sec:Applications}
    \input{applications}

    \vspace{15pt}


    \section{Contribution Statement}
    \label{sec:contribution-statement}
    \begin{itemize}
        \item Abel Haris Harsono: 33.3\%
        \item Arnav Varshney: 33.3\%
        \item Filbert David Tejalaksana: 33.3\%
    \end{itemize}

    \vspace{15pt}
    \label{itm:refs}
    \input{refs}

\end{document}
