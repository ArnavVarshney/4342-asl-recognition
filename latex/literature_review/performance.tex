\subsection{Introduction}
\label{subsec:introduction3}

Over the years, GPUs have served as the primary hardware accelerators for deep learning workloads,
offering significant performance improvements over traditional CPUs.
This has been further aided by the development of specialized software for high-performance deep learning such as cuDNN and cuBLAS for NVIDIA GPUs and ROCm for AMD GPUs.
Only recently, however, TPUs (and TensorFlow) have emerged as a viable alternative to GPUs for deep learning tasks.
However,
different generations of AI accelerators across different vendors exhibit vastly different performance and energy consumption characteristics.
Even with similar on-paper specifications like core clocks, Video random-access memory (VRAM), memory bandwidth,
the real-life performance relies heavily on an accelerator's underlying architecture and
software (driver) optimizations.

Additionally, for server-based deployment,
monitoring energy consumption has been vital for DNN training,
since long-term electric bills can directly benefit from lower energy consumption.
There exist techniques to balance performance and energy consumption such as dynamic voltage and frequency
(DVFS)\cite{b1} and job scheduling algorithms like First Come, First Served (FCFS) or Shortest Job First (SJF)\cite{b2}, etc.

Most existing benchmarks exclusively consider either performance or energy consumption for particular accelerators and/or algorithms.
Also, there is limited data available for AMD GPUs, despite significant improvements to the ROCm ecosystem.
Hence, for the purposes of this work,
wide benchmarks for CIOs and real-world tasks such as image recognition,
speech recognition and translation tasks are made.

